{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Installing Necessary Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:14:17.422146Z","iopub.status.busy":"2024-04-26T15:14:17.421757Z","iopub.status.idle":"2024-04-26T15:14:55.580379Z","shell.execute_reply":"2024-04-26T15:14:55.579092Z","shell.execute_reply.started":"2024-04-26T15:14:17.422112Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  Running command git clone --filter=blob:none --quiet https://github.com/leimao/cocoapi.git /tmp/pip-req-build-semk52up\n"]}],"source":["# Installing the COCO API\n","! pip install -U 'git+https://github.com/leimao/cocoapi.git#subdirectory=PythonAPI' > /dev/null\n","\n","# For pretrained DeiT Transformer\n","! pip install timm > /dev/null"]},{"cell_type":"markdown","metadata":{},"source":["### Necessary Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:14:55.583031Z","iopub.status.busy":"2024-04-26T15:14:55.582665Z","iopub.status.idle":"2024-04-26T15:15:07.229211Z","shell.execute_reply":"2024-04-26T15:15:07.228398Z","shell.execute_reply.started":"2024-04-26T15:14:55.582998Z"},"trusted":true},"outputs":[],"source":["import torch\n","import os\n","import random\n","import numpy as np \n","import nltk\n","import pickle\n","import timm\n","import time\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm\n","from pycocotools.coco import COCO\n","from collections import Counter\n","from torchvision.transforms import Compose,ToTensor,RandomResizedCrop,RandomHorizontalFlip,Resize\n","from torch.utils.data import Dataset,DataLoader\n","from typing import Optional,Callable\n","from PIL import Image\n","from torch.nn.utils.rnn import pad_sequence\n","from torch import nn\n","from torch import optim\n","from timm.models.layers import trunc_normal_\n","from torchmetrics import Metric\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"markdown","metadata":{},"source":[" ### Global"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.230650Z","iopub.status.busy":"2024-04-26T15:15:07.230376Z","iopub.status.idle":"2024-04-26T15:15:07.259741Z","shell.execute_reply":"2024-04-26T15:15:07.258508Z","shell.execute_reply.started":"2024-04-26T15:15:07.230625Z"},"trusted":true},"outputs":[],"source":["class GLOBAL:\n","    \n","    working_dir = \"/kaggle/working\"\n","    input_dir = \"/kaggle/input\"\n","    \n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    \n","    max_len = 100\n","    img_size = 384\n","    num_bins = img_size\n","    num_classes = 91\n","    word_threshold = 20\n","    num_patches = 576\n","    \n","    feature_extractor_name = \"deit3_small_patch16_384.fb_in22k_ft_in1k\"\n","    dataset_root = os.path.join(input_dir, \"coco-2017-dataset\", \"coco2017\")\n","    caption_path = os.path.join(dataset_root, 'annotations', 'captions_train2017.json')\n","    vocab_path = os.path.join(working_dir, \"vocab.pkl\")\n","    weights_dir = os.path.join(working_dir, \"weights\")\n","    \n","    lr = 10e-4\n","    weight_decay = 10e-4\n","    start_epoch = 0\n","    batch_size = 16\n","    epochs = 2\n","    features_dim = 256\n","    \n","    num_workers = 2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.263102Z","iopub.status.busy":"2024-04-26T15:15:07.262516Z","iopub.status.idle":"2024-04-26T15:15:07.279454Z","shell.execute_reply":"2024-04-26T15:15:07.278565Z","shell.execute_reply.started":"2024-04-26T15:15:07.263067Z"},"trusted":true},"outputs":[],"source":["class FLAGS:\n","    \n","    build_vocab = False\n","    train = True"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.280868Z","iopub.status.busy":"2024-04-26T15:15:07.280559Z","iopub.status.idle":"2024-04-26T15:15:07.289586Z","shell.execute_reply":"2024-04-26T15:15:07.288719Z","shell.execute_reply.started":"2024-04-26T15:15:07.280844Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(GLOBAL.weights_dir):\n","    os.mkdir(GLOBAL.weights_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Reproducibility"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.290889Z","iopub.status.busy":"2024-04-26T15:15:07.290604Z","iopub.status.idle":"2024-04-26T15:15:07.300654Z","shell.execute_reply":"2024-04-26T15:15:07.299531Z","shell.execute_reply.started":"2024-04-26T15:15:07.290867Z"},"trusted":true},"outputs":[],"source":["SEED = 50\n","\n","torch.cuda.manual_seed(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.enabled = False"]},{"cell_type":"markdown","metadata":{},"source":["### Vocabulary"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.302793Z","iopub.status.busy":"2024-04-26T15:15:07.302429Z","iopub.status.idle":"2024-04-26T15:15:07.311495Z","shell.execute_reply":"2024-04-26T15:15:07.310633Z","shell.execute_reply.started":"2024-04-26T15:15:07.302761Z"},"trusted":true},"outputs":[],"source":["class Vocab:\n","    \n","    def __init__(self):\n","        \n","        self.str_2_idx = {}\n","        self.idx_2_str = {}\n","        self.idx = 0\n","        \n","        self.unk_token = '<unk>'\n","        \n","        self.add_word(self.unk_token)\n","                \n","    def add_word(self, word : str):\n","        \n","        if word not in self.str_2_idx:\n","            self.str_2_idx[word] = self.idx\n","            self.idx_2_str[self.idx] = word\n","            self.idx += 1\n","            \n","    def __call__(self, word : str):\n","        \n","        if word not in self.str_2_idx:\n","            return self.str_2_idx[self.unk_token]\n","        \n","        return self.str_2_idx[word]\n","    \n","    def get_word(self, index : int):\n","        \n","        if index not in self.idx_2_str:\n","            return self.unk_token\n","        \n","        return self.idx_2_str[index]\n","    \n","    def __len__(self):\n","        return len(self.str_2_idx)\n","        "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.313030Z","iopub.status.busy":"2024-04-26T15:15:07.312687Z","iopub.status.idle":"2024-04-26T15:15:07.324197Z","shell.execute_reply":"2024-04-26T15:15:07.323381Z","shell.execute_reply.started":"2024-04-26T15:15:07.312999Z"},"trusted":true},"outputs":[],"source":["def build_vocab(json : str, word_threshold : int):\n","    \n","    coco = COCO(json)\n","    counter = Counter()\n","    ids = coco.anns.keys()\n","    \n","    for i,id in tqdm(enumerate(ids), total=len(ids)):\n","        caption = str(coco.anns[id]['caption'])\n","        tokens = nltk.tokenize.word_tokenize(caption.lower()) \n","        counter.update(tokens)\n","        \n","    words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n","    \n","    vocab = Vocab()\n","\n","    # Add the words to the vocabulary.\n","    for i, word in tqdm(enumerate(words), total=len(words)):\n","        vocab.add_word(word)\n","        \n","    return vocab"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.325419Z","iopub.status.busy":"2024-04-26T15:15:07.325150Z","iopub.status.idle":"2024-04-26T15:15:07.335854Z","shell.execute_reply":"2024-04-26T15:15:07.335117Z","shell.execute_reply.started":"2024-04-26T15:15:07.325388Z"},"trusted":true},"outputs":[],"source":["def load_or_build_vocab(\n","    build : bool,\n","    caption_path : str,\n","    word_threshold : int,\n","    vocab_path : str,\n","    verbose : bool = True\n",") -> Vocab:\n","    \n","    if build:\n","        \n","        if verbose:\n","            print(f\"-- Building vocabulary --\")\n","    \n","        vocab = build_vocab(caption_path, word_threshold)\n","\n","        with open(vocab_path, 'wb') as f:\n","            pickle.dump(vocab, f)\n","            \n","        if verbose:\n","            print(f\"-- Vocabulary size = {len(vocab)} --\")\n","            print(f\"-- Vocabulary saved to = {vocab_path} --\")\n","            \n","        return vocab\n","                    \n","    if verbose:\n","        print(f\"-- Loading vocabulary from path : {vocab_path} --\")\n","            \n","    vocab = None\n","        \n","    with open(vocab_path, 'rb') as f:\n","        vocab = pickle.load(f)\n","            \n","    return vocab"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:07.339570Z","iopub.status.busy":"2024-04-26T15:15:07.339316Z","iopub.status.idle":"2024-04-26T15:15:08.396529Z","shell.execute_reply":"2024-04-26T15:15:08.395363Z","shell.execute_reply.started":"2024-04-26T15:15:07.339549Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["state.db  vocab.pkl  weights\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.398404Z","iopub.status.busy":"2024-04-26T15:15:08.398093Z","iopub.status.idle":"2024-04-26T15:15:08.408910Z","shell.execute_reply":"2024-04-26T15:15:08.407795Z","shell.execute_reply.started":"2024-04-26T15:15:08.398372Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Loading vocabulary from path : /kaggle/working/vocab.pkl --\n"]}],"source":["vocab = load_or_build_vocab(FLAGS.build_vocab, GLOBAL.caption_path, GLOBAL.word_threshold, GLOBAL.vocab_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.410443Z","iopub.status.busy":"2024-04-26T15:15:08.410091Z","iopub.status.idle":"2024-04-26T15:15:08.421110Z","shell.execute_reply":"2024-04-26T15:15:08.420257Z","shell.execute_reply.started":"2024-04-26T15:15:08.410416Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    \n","    def __init__(self, num_classes: int, \n","        num_bins: int, \n","        width: int, \n","        height: int, \n","        max_len : int\n","    ):\n","        self.num_classes = num_classes\n","        self.num_bins = num_bins\n","        self.width = width\n","        self.height = height\n","        self.max_len = max_len\n","        self.max_len_obj = int((self.max_len - 2) / 5)\n","        self.BOS_code = num_classes + num_bins \n","        self.EOS_code = self.BOS_code + 1\n","        self.PAD_code = self.EOS_code + 1\n","        self.text_id_shift = 550\n","        self.vocab_size = 6000 "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.422571Z","iopub.status.busy":"2024-04-26T15:15:08.422278Z","iopub.status.idle":"2024-04-26T15:15:08.430218Z","shell.execute_reply":"2024-04-26T15:15:08.429338Z","shell.execute_reply.started":"2024-04-26T15:15:08.422547Z"},"trusted":true},"outputs":[],"source":["tokenizer = Tokenizer(\n","    num_classes=GLOBAL.num_classes, \n","    num_bins=GLOBAL.num_bins,\n","    width=GLOBAL.img_size, \n","    height=GLOBAL.img_size, \n","    max_len=GLOBAL.max_len\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.431758Z","iopub.status.busy":"2024-04-26T15:15:08.431384Z","iopub.status.idle":"2024-04-26T15:15:08.438761Z","shell.execute_reply":"2024-04-26T15:15:08.437919Z","shell.execute_reply.started":"2024-04-26T15:15:08.431734Z"},"trusted":true},"outputs":[],"source":["def create_transfroms(size : int) -> tuple[Compose,Compose]:\n","    \n","    train_transfroms = Compose([\n","        RandomHorizontalFlip(p=0.5),\n","        RandomResizedCrop(size=size),\n","        ToTensor()\n","    ])\n","    \n","    val_transfroms = Compose([\n","        RandomResizedCrop(size=size),\n","        ToTensor()\n","    ])\n","    \n","    return train_transfroms,val_transfroms"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.440427Z","iopub.status.busy":"2024-04-26T15:15:08.440040Z","iopub.status.idle":"2024-04-26T15:15:08.452841Z","shell.execute_reply":"2024-04-26T15:15:08.451959Z","shell.execute_reply.started":"2024-04-26T15:15:08.440396Z"},"trusted":true},"outputs":[],"source":["class CaptionPreprocessor:\n","    \n","    def __init__(self,vocab : Vocab, tokenizer : Tokenizer):\n","        self.vocab = vocab\n","        self.tokenizer = tokenizer\n","    \n","    def __call__(self, caption : str):\n","        \n","        tokenizer = self.tokenizer\n","        vocab = self.vocab\n","    \n","        tokens = nltk.tokenize.word_tokenize(caption.lower()) \n","        tokens = [tokenizer.BOS_code] + [vocab(token) + tokenizer.text_id_shift for token in tokens] + [tokenizer.EOS_code]\n","        return torch.tensor(tokens).type(torch.long)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.454144Z","iopub.status.busy":"2024-04-26T15:15:08.453862Z","iopub.status.idle":"2024-04-26T15:15:08.467124Z","shell.execute_reply":"2024-04-26T15:15:08.466207Z","shell.execute_reply.started":"2024-04-26T15:15:08.454122Z"},"trusted":true},"outputs":[],"source":["class COCODataset(Dataset):\n","    \n","    def __init__(self,\n","        root : str,\n","        split : str,\n","        img_transfroms : Optional[Callable] = None,\n","        caption_transfroms : Optional[Callable] = None\n","    ):\n","        \n","        self.root = root\n","        self.annot_filename = os.path.join(root, \"annotations\", f\"captions_{split}2017.json\")\n","        self.img_root = os.path.join(root, f\"{split}2017\")\n","        \n","        if not os.path.exists(self.root):\n","            raise Exception(f\"{self.root} doesn't exist.\")\n","            \n","        if not os.path.exists(self.annot_filename):\n","            raise Exception(f\"{self.annot_filename} doesn't exist.\")\n","            \n","        if not os.path.exists(self.img_root):\n","            raise Exception(f\"{self.img_root} doesn't exist.\")\n","            \n","        self.coco = COCO(self.annot_filename)\n","        self.ids = list(self.coco.anns.keys())\n","        self.caption_transfroms = caption_transfroms\n","        self.img_transfroms = img_transfroms\n","    \n","    def __getitem__(self, index : int):\n","        \n","        caption_id = self.ids[index]\n","        caption = self.coco.anns[caption_id]['caption']\n","        \n","        img_id = self.coco.anns[caption_id]['image_id']\n","        path = self.coco.loadImgs(img_id)[0]['file_name']\n","        path = os.path.join(self.img_root, path)\n","        \n","        img = Image.open(path).convert('RGB')\n","        \n","        if self.img_transfroms is not None:\n","            img = self.img_transfroms(img)\n","            \n","        if self.caption_transfroms is not None:\n","            caption = self.caption_transfroms(caption)\n","            \n","        return img,caption\n","    \n","    def __len__(self) -> int:\n","        return len(self.ids)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.468367Z","iopub.status.busy":"2024-04-26T15:15:08.468116Z","iopub.status.idle":"2024-04-26T15:15:08.482413Z","shell.execute_reply":"2024-04-26T15:15:08.481682Z","shell.execute_reply.started":"2024-04-26T15:15:08.468346Z"},"trusted":true},"outputs":[],"source":["class Collate:\n","    \n","    def __init__(self,pad_idx : int,max_len : Optional[int]):\n","        self.pad_idx = pad_idx\n","        self.max_len = max_len\n","    \n","    def __call__(self, batch : list[tuple[torch.Tensor,torch.Tensor]]):\n","        \n","        imgs = torch.stack([row[0] for row in batch])\n","        captions = [row[1] for row in batch]\n","        \n","        captions = pad_sequence(captions, padding_value=self.pad_idx,batch_first=True)\n","        \n","        if self.max_len is not None:\n","            pad = torch.ones(captions.size(0), self.max_len - captions.size(1)).fill_(self.pad_idx).type(torch.long)\n","            captions = torch.cat([captions, pad], dim=1)\n","            \n","        return imgs,captions"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.483900Z","iopub.status.busy":"2024-04-26T15:15:08.483533Z","iopub.status.idle":"2024-04-26T15:15:08.497420Z","shell.execute_reply":"2024-04-26T15:15:08.496609Z","shell.execute_reply.started":"2024-04-26T15:15:08.483871Z"},"trusted":true},"outputs":[],"source":["def create_dataloaders(\n","    dataset_root : str,\n","    img_size : int,\n","    batch_size : int,\n","    num_workers : int,\n","    max_len : int,\n","    vocab : Vocab,\n","    tokenizer : Tokenizer\n",") -> tuple[DataLoader,DataLoader]:\n","    \n","    train_transfroms, val_transfroms = create_transfroms(img_size)\n","    \n","    train_caption_transfroms = CaptionPreprocessor(vocab, tokenizer)\n","    val_caption_transfroms = CaptionPreprocessor(vocab, tokenizer)\n","    \n","    train_collate = Collate(tokenizer.PAD_code,max_len)\n","    val_collate = Collate(tokenizer.PAD_code,max_len)\n","\n","    train_data = COCODataset(dataset_root,split='train',img_transfroms=train_transfroms, caption_transfroms=train_caption_transfroms)\n","    val_data = COCODataset(dataset_root,split='val',img_transfroms=val_transfroms, caption_transfroms=val_caption_transfroms)\n","    \n","    train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,collate_fn=train_collate)\n","    val_loader = DataLoader(dataset=val_data,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,collate_fn=val_collate)\n","    \n","    return train_loader,val_loader"]},{"cell_type":"markdown","metadata":{},"source":["### Architecture"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.499011Z","iopub.status.busy":"2024-04-26T15:15:08.498629Z","iopub.status.idle":"2024-04-26T15:15:08.508557Z","shell.execute_reply":"2024-04-26T15:15:08.507631Z","shell.execute_reply.started":"2024-04-26T15:15:08.498962Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","        \n","    def __init__(self, model_name, out_dim : int, pretrained : bool = True):\n","        super().__init__()\n","        \n","        self.out_dim = out_dim\n","\n","        self.model = timm.create_model(model_name, num_classes=0, \n","                                           global_pool='', pretrained=pretrained)\n","        self.bottleneck = nn.AdaptiveAvgPool1d(out_dim)\n","\n","    def forward(self, x):\n","        features = self.model(x)\n","        return self.bottleneck(features[:, 1:])"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.510485Z","iopub.status.busy":"2024-04-26T15:15:08.509870Z","iopub.status.idle":"2024-04-26T15:15:08.523865Z","shell.execute_reply":"2024-04-26T15:15:08.522987Z","shell.execute_reply.started":"2024-04-26T15:15:08.510452Z"},"trusted":true},"outputs":[],"source":["class PosEmbeddings(nn.Module):\n","    \n","    def __init__(self, max_len : int,dim : int):\n","        super().__init__()\n","        \n","        self.max_len = max_len\n","        self.dim = dim\n","        \n","        self.weight = nn.Parameter(torch.randn(1, max_len, dim) * .02)\n","        \n","        self.init_weights()\n","        \n","    def forward(self, x):\n","        return x + self.weight\n","    \n","    def init_weights(self):\n","        trunc_normal_(self.weight, std=.02)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.526216Z","iopub.status.busy":"2024-04-26T15:15:08.524996Z","iopub.status.idle":"2024-04-26T15:15:08.534372Z","shell.execute_reply":"2024-04-26T15:15:08.533487Z","shell.execute_reply.started":"2024-04-26T15:15:08.526191Z"},"trusted":true},"outputs":[],"source":["class Mask(nn.Module):\n","    \n","    def __init__(self, \n","        device : torch.device,\n","        pad_idx : int\n","    ):\n","        super().__init__()\n","        \n","        self.device = device\n","        self.pad_idx = pad_idx\n","        \n","    def forward(self, target):\n","        \n","        target_len = target.shape[1]\n","        \n","        mask = torch.ones(size=(target_len, target_len), device=self.device)\n","        mask = torch.triu(mask)\n","        mask = mask == 1\n","        mask = mask.transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","                \n","        target_padding_mask = (target == self.pad_idx)\n","        \n","        return mask, target_padding_mask"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.535867Z","iopub.status.busy":"2024-04-26T15:15:08.535536Z","iopub.status.idle":"2024-04-26T15:15:08.549365Z","shell.execute_reply":"2024-04-26T15:15:08.548464Z","shell.execute_reply.started":"2024-04-26T15:15:08.535836Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,\n","        vocab_size : int, \n","        encoder_length : int, \n","        dim : int, \n","        max_len : int,\n","        num_heads : int, \n","        num_layers : int,\n","        device : torch.device,\n","        pad_idx : int\n","    ):\n","        super().__init__()\n","        \n","        self.mask = Mask(device,pad_idx)\n","        \n","        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=dim)\n","        \n","        self.decoder_pos_embed = PosEmbeddings(max_len-1, dim)\n","        self.decoder_pos_drop = nn.Dropout(p=0.05)\n","        \n","        decoder_layer = nn.TransformerDecoderLayer(d_model=dim, nhead=num_heads)\n","        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n","        self.output = nn.Linear(dim, vocab_size)\n","        \n","        self.encoder_pos_embed = PosEmbeddings(encoder_length, dim)\n","        self.encoder_pos_drop = nn.Dropout(p=0.05)\n","        \n","        self.init_weights()\n","    \n","    def init_weights(self):\n","        for name, param in self.named_parameters():\n","            if not 'encoder_pos_embed' in name and 'decoder_pos_embed' not in name and param.dim() > 1:\n","                nn.init.xavier_uniform_(param)\n","                \n","    def forward(self, encoder_out, target):\n","                \n","        target_mask, target_padding_mask = self.mask(target)\n","        \n","        target_embedding = self.embedding(target)\n","        target_embedding = self.decoder_pos_embed(target_embedding)\n","        target_embedding = self.decoder_pos_drop(target_embedding)\n","        \n","        encoder_out = self.encoder_pos_embed(encoder_out)\n","        encoder_out = self.encoder_pos_drop(encoder_out)\n","        \n","        encoder_out = encoder_out.transpose(0, 1)\n","        target_embedding = target_embedding.transpose(0, 1)\n","        \n","        preds = self.decoder(\n","            memory=encoder_out,\n","            tgt=target_embedding,\n","            tgt_mask=target_mask,\n","            tgt_key_padding_mask=target_padding_mask.float()\n","        )\n","        \n","        preds = preds.transpose(0, 1)\n","        \n","        outputs = self.output(preds)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.551240Z","iopub.status.busy":"2024-04-26T15:15:08.550729Z","iopub.status.idle":"2024-04-26T15:15:08.562368Z","shell.execute_reply":"2024-04-26T15:15:08.561560Z","shell.execute_reply.started":"2024-04-26T15:15:08.551216Z"},"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    \n","    def __init__(self,\n","        feature_extractor_name : str,\n","        features_dim : int,\n","        vocab_size : int,\n","        encoder_length : int,\n","        num_heads : int,\n","        num_layers : int,\n","        max_len : int,\n","        device : torch.device,\n","        pad_idx : int\n","    ):\n","        super().__init__()\n","        \n","        self.feature_extractor_name = feature_extractor_name\n","        self.features_dim = features_dim\n","        self.vocab_size = vocab_size\n","        self.encoder_length = encoder_length\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","        self.max_len = max_len\n","        self.device = device\n","        self.pad_idx = pad_idx\n","        \n","        self.encoder = Encoder(\n","            model_name=self.feature_extractor_name,\n","            out_dim=features_dim,\n","            pretrained=True\n","        )\n","        \n","        self.decoder = Decoder(\n","            vocab_size=self.vocab_size,\n","            encoder_length=self.encoder_length,\n","            dim=self.features_dim,\n","            max_len=self.max_len,\n","            num_heads=self.num_heads,\n","            num_layers=self.num_layers,\n","            device=self.device,\n","            pad_idx=self.pad_idx\n","        )\n","    \n","    def forward(self, image, target):\n","        encoder_out = self.encoder(image)\n","        preds = self.decoder(encoder_out, target)\n","        return preds"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.563855Z","iopub.status.busy":"2024-04-26T15:15:08.563554Z","iopub.status.idle":"2024-04-26T15:15:08.575144Z","shell.execute_reply":"2024-04-26T15:15:08.574391Z","shell.execute_reply.started":"2024-04-26T15:15:08.563832Z"},"trusted":true},"outputs":[],"source":["class Seq2SeqCrossEntropyLoss(nn.CrossEntropyLoss):\n","    \n","    def forward(self, y_hat : torch.Tensor, y : torch.Tensor) -> torch.Tensor:\n","        \n","        y_hat = y_hat.reshape(-1, y_hat.size(-1))\n","        y = y.reshape(-1)\n","        \n","        return super().forward(y_hat, y)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.576640Z","iopub.status.busy":"2024-04-26T15:15:08.576323Z","iopub.status.idle":"2024-04-26T15:15:08.585533Z","shell.execute_reply":"2024-04-26T15:15:08.584685Z","shell.execute_reply.started":"2024-04-26T15:15:08.576617Z"},"trusted":true},"outputs":[],"source":["def read_weights_folder(path : str):\n","    \n","    folders = os.listdir(path)\n","    folders = filter(lambda f : f.endswith('.pt'),folders)\n","    folders = map(lambda f : os.path.join(path, f), folders)\n","    folders = sorted(folders)\n","    num_epochs = len(folders)\n","    \n","    last_weights = None\n","    \n","    if num_epochs != 0:\n","        last_weights = torch.load(folders[-1])\n","        \n","    return last_weights,num_epochs"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.587628Z","iopub.status.busy":"2024-04-26T15:15:08.586815Z","iopub.status.idle":"2024-04-26T15:15:08.599771Z","shell.execute_reply":"2024-04-26T15:15:08.598854Z","shell.execute_reply.started":"2024-04-26T15:15:08.587602Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0 False\n"]}],"source":["weights, num_epochs = read_weights_folder(GLOBAL.weights_dir)\n","print(num_epochs,weights != None)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:08.601185Z","iopub.status.busy":"2024-04-26T15:15:08.600861Z","iopub.status.idle":"2024-04-26T15:15:11.587353Z","shell.execute_reply":"2024-04-26T15:15:11.586459Z","shell.execute_reply.started":"2024-04-26T15:15:08.601162Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=2.14s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.11s)\n","creating index...\n","index created!\n"]}],"source":["train_loader,val_loader = create_dataloaders(\n","    dataset_root=GLOBAL.dataset_root,\n","    img_size=GLOBAL.img_size,\n","    batch_size=GLOBAL.batch_size,\n","    num_workers=GLOBAL.num_workers,\n","    max_len=GLOBAL.max_len,\n","    vocab=vocab,\n","    tokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:11.594130Z","iopub.status.busy":"2024-04-26T15:15:11.593809Z","iopub.status.idle":"2024-04-26T15:15:12.567785Z","shell.execute_reply":"2024-04-26T15:15:12.566465Z","shell.execute_reply.started":"2024-04-26T15:15:11.594103Z"},"trusted":true},"outputs":[],"source":["x, y = next(iter(train_loader))\n","x, y = x.to(GLOBAL.device), y.to(GLOBAL.device)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:12.569851Z","iopub.status.busy":"2024-04-26T15:15:12.569459Z","iopub.status.idle":"2024-04-26T15:15:14.917269Z","shell.execute_reply":"2024-04-26T15:15:14.916422Z","shell.execute_reply.started":"2024-04-26T15:15:12.569809Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9c1ac4aefe848d893d3b866cc918cdd","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/88.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = EncoderDecoder(\n","    feature_extractor_name = GLOBAL.feature_extractor_name,\n","    features_dim = GLOBAL.features_dim,\n","    vocab_size = tokenizer.vocab_size,\n","    encoder_length = GLOBAL.num_patches,\n","    num_heads = 8,\n","    num_layers = 6,\n","    max_len = GLOBAL.max_len,\n","    device = GLOBAL.device,\n","    pad_idx = tokenizer.PAD_code\n",").to(GLOBAL.device)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.919296Z","iopub.status.busy":"2024-04-26T15:15:14.918632Z","iopub.status.idle":"2024-04-26T15:15:14.926640Z","shell.execute_reply":"2024-04-26T15:15:14.925611Z","shell.execute_reply.started":"2024-04-26T15:15:14.919257Z"},"trusted":true},"outputs":[],"source":["if weights is not None:\n","    model.load_state_dict(weights)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.928846Z","iopub.status.busy":"2024-04-26T15:15:14.928057Z","iopub.status.idle":"2024-04-26T15:15:14.936994Z","shell.execute_reply":"2024-04-26T15:15:14.936254Z","shell.execute_reply.started":"2024-04-26T15:15:14.928811Z"},"trusted":true},"outputs":[],"source":["num_training_steps = GLOBAL.epochs * len(train_loader)\n","num_warmup_steps = int(0.05 * num_training_steps)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.938543Z","iopub.status.busy":"2024-04-26T15:15:14.938199Z","iopub.status.idle":"2024-04-26T15:15:14.950491Z","shell.execute_reply":"2024-04-26T15:15:14.949546Z","shell.execute_reply.started":"2024-04-26T15:15:14.938514Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.AdamW(params=model.parameters(), lr=GLOBAL.lr, weight_decay=GLOBAL.weight_decay)\n","criterion = Seq2SeqCrossEntropyLoss(ignore_index=tokenizer.PAD_code).to(GLOBAL.device)\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n","        num_training_steps=num_training_steps,\n","        num_warmup_steps=num_warmup_steps,\n","        last_epoch=num_epochs-1\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.952503Z","iopub.status.busy":"2024-04-26T15:15:14.952142Z","iopub.status.idle":"2024-04-26T15:15:14.979916Z","shell.execute_reply":"2024-04-26T15:15:14.979140Z","shell.execute_reply.started":"2024-04-26T15:15:14.952471Z"},"trusted":true},"outputs":[],"source":["class Trainer:\n","    \n","    def __init__(self,\n","        model : nn.Module,\n","        criterion : nn.Module,\n","        optimizer : optim.Optimizer,\n","        device : torch.device,\n","        weights_folder : str,\n","        num_epochs : int,\n","        lr_scheduler : Optional[optim.lr_scheduler.LRScheduler] = None,        \n","    ):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","        self.lr_scheduler = lr_scheduler\n","        self.history = self.init_history()\n","        self.weights_folder = weights_folder\n","        self.num_epochs = num_epochs\n","        \n","    def init_history(self) -> dict:\n","        \n","        metrics = ['loss','epoch','time']\n","        \n","        history = {}\n","        \n","        for split in ['train','val']:\n","            \n","            history[split] = {}\n","                    \n","            for metric in metrics:\n","                history[split][metric] = []\n","                \n","        return history\n","        \n","    def train_on_batch(self, train_batch : tuple[torch.Tensor,torch.Tensor]) -> tuple[torch.Tensor,torch.Tensor]:\n","            \n","        ### put the data in the appropriate device\n","        x, y = train_batch\n","        x, y = x.to(self.device), y.to(self.device)\n","        \n","        y = y[:,:-1]\n","        \n","        ### forward pass\n","        y_hat = self.model(x, y)\n","        \n","        ### loss\n","        loss = self.criterion(y_hat, y)\n","        \n","        ### zero the gradients (they accumelate by default)\n","        self.optimizer.zero_grad()\n","        \n","        ### backward step\n","        loss.backward()\n","        \n","        ### update the weights\n","        self.optimizer.step()\n","        \n","        ### update learning rate\n","        if self.lr_scheduler is not None:\n","            self.lr_scheduler.step()\n","        \n","        return y_hat,loss\n","    \n","    def train_step(self, data_loader : DataLoader, epoch_num : int) -> None:\n","        \n","        ### put the model in training mode\n","        self.model.train()\n","        \n","        t_object = tqdm(data_loader, total=len(data_loader))\n","        \n","        running_loss = 0.0\n","        \n","        tic = time.time()\n","        \n","        for train_batch in t_object:\n","            \n","            y_hat, loss = self.train_on_batch(train_batch)\n","            \n","            current_loss = loss.item()\n","            running_loss += current_loss\n","            \n","            t_object.set_description(f\"Epoch {epoch_num+1} : loss = {current_loss}\")\n","        \n","        toc = time.time()\n","        \n","        running_loss = running_loss / len(data_loader)\n","        epoch_time = toc - tic\n","        \n","        self.history[\"train\"][\"loss\"].append(running_loss)\n","        self.history[\"train\"][\"epoch\"].append(epoch_num)\n","        self.history[\"train\"][\"time\"].append(epoch_time)\n","    \n","    def val_on_batch(self, train_batch : tuple[torch.Tensor,torch.Tensor]) -> tuple[torch.Tensor,torch.Tensor]:\n","        \n","        ### put the data in the appropriate device\n","        x, y = train_batch\n","        x, y = x.to(self.device), y.to(self.device)\n","        \n","        y_input = y[:,:-1]\n","        y_expected = y[:,1:]\n","        \n","        ### forward pass\n","        y_hat = self.model(x, y_input)\n","        \n","        ### loss\n","        loss = self.criterion(y_hat, y_expected)\n","        \n","        return y_hat,loss\n","    \n","    def val_step(self, data_loader : DataLoader, epoch_num : int) -> None:\n","        \n","        ### put the model in evaluation mode\n","        self.model.eval()\n","        \n","        with torch.inference_mode():\n","            \n","            t_object = tqdm(data_loader, total=len(data_loader))\n","\n","            running_loss = 0.0\n","\n","            tic = time.time()\n","\n","            for val_batch in t_object:\n","\n","                y_hat, loss = self.val_on_batch(val_batch)\n","                running_loss += loss.item()\n","\n","            toc = time.time()\n","\n","            running_loss = running_loss / len(data_loader)\n","            epoch_time = toc - tic\n","\n","            self.history[\"val\"][\"loss\"].append(running_loss)\n","            self.history[\"val\"][\"epoch\"].append(epoch_num)\n","            self.history[\"val\"][\"time\"].append(epoch_time)\n","        \n","    def train(self,\n","        train_loader : DataLoader,\n","        val_loader : DataLoader,\n","        epochs : int = 1\n","    ) -> None:\n","        \n","        for epoch in range(epochs):\n","            \n","            self.train_step(train_loader, epoch)\n","            self.val_step(val_loader, epoch)\n","            \n","            last_train_loss = self.history[\"train\"][\"loss\"][-1]\n","            last_val_loss = self.history[\"val\"][\"loss\"][-1]\n","            \n","            path = os.path.join(self.weights_folder, f\"epoch_{self.num_epochs+epoch}.pt\")\n","            torch.save(model.state_dict(), f=path)\n","            \n","            print(f\"model saved to = {path}\")\n","            print(f\"Epoch = {epoch+1} : train_loss = {last_train_loss},train_loss = {last_val_loss}\")"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.981250Z","iopub.status.busy":"2024-04-26T15:15:14.980952Z","iopub.status.idle":"2024-04-26T15:15:14.992615Z","shell.execute_reply":"2024-04-26T15:15:14.991719Z","shell.execute_reply.started":"2024-04-26T15:15:14.981226Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    lr_scheduler=lr_scheduler,\n","    device=GLOBAL.device,\n","    num_epochs=num_epochs,\n","    weights_folder=GLOBAL.weights_dir\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T15:15:14.994005Z","iopub.status.busy":"2024-04-26T15:15:14.993712Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"163e17bc6c514941b5ec84e5d4e442c4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/36985 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["if FLAGS.train:    \n","    trainer.train(train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.listdir(GLOBAL.weights_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def top_k_top_p_filtering(\n","    logits: torch.Tensor,\n","    top_k: int = 0,\n","    top_p: float = 1.0,\n","    filter_value: float = -float(\"Inf\"),\n","    min_tokens_to_keep: int = 1,\n",") -> torch.Tensor:\n","    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n","    Args:\n","        logits: logits distribution shape (batch size, vocabulary size)\n","        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n","        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n","            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n","        Make sure we keep at least min_tokens_to_keep per batch example in the output\n","    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n","    \"\"\"\n","    if top_k > 0:\n","        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n","        # Remove all tokens with a probability less than the last token of the top-k\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p < 1.0:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        if min_tokens_to_keep > 1:\n","            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n","            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        # scatter sorted tensors to original indexing\n","        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n","        logits[indices_to_remove] = filter_value\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate_caption(\n","    model : nn.Module,\n","    image : Image.Image,\n","    tokenizer : Tokenizer,\n","    device : torch.device,\n","    size : int,\n","    max_len : int = 50,\n","    top_k : int = 0,\n","    top_p : float = 1,\n","):\n","    \n","    ### preprocessing the image\n","    preprocessor = Compose([\n","        Resize((size,size)),\n","        ToTensor()\n","    ])\n","    \n","    x = preprocessor(image).unsqueeze(0).to(device)\n","        \n","    batch_preds = torch.ones(x.size(0), 1).fill_(tokenizer.BOS_code).long().to(device)\n","    confs = []\n","    \n","    if top_k != 0 or top_p != 1:\n","        sample = lambda preds: torch.softmax(preds, dim=-1).multinomial(num_samples=1).view(-1, 1)\n","    else:\n","        sample = lambda preds: torch.softmax(preds, dim=-1).argmax(dim=-1).view(-1, 1)\n","    \n","    ### prediction\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        \n","        for i in tqdm(range(max_len)):\n","            \n","            encoder_out = model.encoder(x)\n","\n","            length = batch_preds.size(1)\n","            padding = torch.ones(batch_preds.size(0), max_len-length).fill_(tokenizer.PAD_code).long().to(device)\n","            tgt = torch.cat([batch_preds, padding], dim=1)\n","\n","            preds = model.decoder(encoder_out, tgt)\n","            preds = preds[:, length-1, :]\n","\n","            preds = top_k_top_p_filtering(preds, top_k=top_k, top_p=top_p)\n","\n","            preds = sample(preds)\n","            batch_preds = torch.cat([batch_preds, preds], dim=1)\n","        \n","    return batch_preds.cpu()\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def postprocess_caption(batch_preds : torch.Tensor, tokenizer : Tokenizer, vocab : Vocab):\n","    \n","    batch_preds[:,-1] = tokenizer.EOS_code \n","    \n","    EOS_idxs = (batch_preds == tokenizer.EOS_code).float().argmax(dim=-1)\n","    \n","    captions = []\n","    \n","    for i, EOS_idx in enumerate(EOS_idxs.tolist()):\n","        \n","        if EOS_idx == 0 or EOS_idx == 1:\n","            \n","            captions.append(None)\n","            continue\n","            \n","        caption = []\n","        \n","        for word in batch_preds[i][1:EOS_idx]:\n","            caption.append(vocab.get_word(word.item()-tokenizer.text_id_shift))\n","        captions.append(caption)\n","        \n","    return captions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate_k_captions(\n","    model : nn.Module,\n","    image : Image.Image,\n","    tokenizer : Tokenizer,\n","    device : torch.device,\n","    size : int,\n","    vocab : Vocab,\n","    max_len : int,\n","    top_k : int = 0,\n","    top_p : float = 1,\n","    k = 5\n","):\n","    \n","    captions = []\n","    \n","    for i in range(k):\n","    \n","        ids = generate_caption(model, img, tokenizer, GLOBAL.device, GLOBAL.img_size,GLOBAL.max_len-1, 20, 0.65)\n","        caption = postprocess_caption(ids, tokenizer, vocab)[0]\n","        caption[0] = caption[0].title()\n","        caption = [word for word in caption if word.lower() != vocab.unk_token]\n","        caption = ' '.join(caption)\n","        captions.append(caption)\n","        \n","    return captions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path = \"/kaggle/input/coco-2017-dataset/coco2017/test2017/000000000001.jpg\"\n","img = Image.open(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generate_k_captions(model, img, tokenizer, GLOBAL.device, GLOBAL.img_size, vocab, GLOBAL.max_len, 20, 0.65)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":857191,"sourceId":1462296,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
